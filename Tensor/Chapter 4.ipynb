{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0238901]\n",
      " [1.0502261]]\n"
     ]
    }
   ],
   "source": [
    "#深层神经网络\n",
    "import tensorflow as tf\n",
    "from numpy.random import  RandomState\n",
    "\n",
    "batch_size=8\n",
    "\n",
    "x=tf.placeholder(tf.float32,shape=[None,2],name='x-input')\n",
    "y_=tf.placeholder(tf.float32,shape=[None,1],name='cost')\n",
    "\n",
    "w1=tf.Variable(tf.random_normal([2,1],stddev=1,seed=1))\n",
    "y=tf.matmul(x,w1)\n",
    "\n",
    "#多预测和少预测的利润\n",
    "loss_less=20\n",
    "loss_more=1\n",
    "'''预测多了少赚1块，预测少了少赚10块'''\n",
    "loss=tf.reduce_sum(tf.where(tf.greater(y,y_),#loss为损失的利润\n",
    "                  (y-y_)*loss_more,\n",
    "                  (y_-y)*loss_less))\n",
    "train_step=tf.train.AdamOptimizer(0.001).minimize(loss)#对损失最小进行训练\n",
    "\n",
    "rdm=RandomState(1)\n",
    "dataSet_size=128\n",
    "X=rdm.rand(dataSet_size,2)\n",
    "#这里需要加上随机数 书上说是因为不同损失函数都会在能够正确预测的时候最低 因此不加入随机数意义不大\n",
    "Y=[[x1+x2+rdm.rand()/10 - 0.05] for (x1,x2 )in X]#默认rand输出随机数在0-1之间\n",
    "'''不加入随机值，那么如果被预测出两个参数的值都是1 1 那么不论什么loss模型，她都可以达到最小损失。\n",
    "添加一些外部干扰，使得机器无法完全拟合模型，数学模型可以在原有模型上下范围波动，那么他就会根据损失有一些趋向，类似于人类智能！！！'''\n",
    "with tf.Session() as sess:\n",
    "    init_op=tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    STEPS=10000\n",
    "    for i in range(STEPS):\n",
    "        start=(i*batch_size)%dataSet_size#取余数\n",
    "        end=min(start+batch_size,dataSet_size)\n",
    "        sess.run(train_step,\n",
    "                feed_dict={x:X[start:end],y_:Y[start:end]})\n",
    "    print(sess.run(w1))\n",
    "    #结果显示机器偏向于多预测一些值 这样可以偏向于多赚一些钱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.586833340656083\n",
      "0.7500217637026599\n",
      "0.8583138364290424\n",
      "0.7550821884676802\n",
      "0.698057248447303\n",
      "0.8644794300545998\n",
      "0.32268099683674645\n",
      "0.6707887907875872\n",
      "0.45087393641334916\n",
      "0.3821027520315172\n",
      "0.4108113499221856\n",
      "0.4014795834695406\n",
      "0.3173839459582769\n",
      "0.6219193679203014\n",
      "0.43024727082126435\n",
      "0.9738020779272523\n",
      "0.6778008914343111\n",
      "0.19856988842711087\n",
      "0.4267010093480328\n",
      "0.343346239774423\n",
      "0.7976388039585666\n",
      "0.8799982885634318\n",
      "0.903841955826372\n",
      "0.6627198123752622\n",
      "0.2702082620297578\n",
      "0.25236670150458973\n",
      "0.8548979426974024\n",
      "0.5277146463087466\n",
      "0.8021610840045981\n",
      "0.5724885171916063\n",
      "0.7331425252875111\n",
      "0.5190116274640558\n",
      "0.770883910501888\n",
      "0.5688579907047155\n",
      "0.4657098785919809\n",
      "0.3426889079532818\n",
      "0.06820934841670412\n",
      "0.37792417932809985\n",
      "0.07962607769825236\n",
      "0.982817113730445\n",
      "0.18161285133076377\n",
      "0.8118586977205398\n",
      "0.8749616449558981\n",
      "0.6884132523859433\n",
      "0.5694944127453757\n",
      "0.16097143681560877\n",
      "0.4668800227633063\n",
      "0.345172051155215\n",
      "0.2250399578140846\n",
      "0.5925118687657968\n",
      "0.31226983770191696\n",
      "0.9163055534683507\n",
      "0.9096355249515571\n",
      "0.2571182937821962\n",
      "0.1108913007440292\n",
      "0.19296273201911285\n",
      "0.49958417067888605\n",
      "0.7285856679745962\n",
      "0.20819443840879148\n",
      "0.24803355837723073\n",
      "0.851671874936367\n",
      "0.41584871826752734\n",
      "0.6166850671552362\n",
      "0.23366613923925006\n",
      "0.10196725942579743\n",
      "0.5158570169685298\n",
      "0.47714098704978236\n",
      "0.15267164409316325\n",
      "0.6218062317404155\n",
      "0.5440101188139381\n",
      "0.6541373469707443\n",
      "0.1445455401246598\n",
      "0.7515278171352439\n",
      "0.2220491397999227\n",
      "0.519351824366033\n",
      "0.7852960282216189\n",
      "0.02233042799180618\n",
      "0.3243624597261865\n",
      "0.872922376400111\n",
      "0.8447096076020696\n",
      "0.5384405925945437\n",
      "0.866608274166513\n",
      "0.9498059913536265\n",
      "0.8264069976293413\n",
      "0.8541154438365387\n",
      "0.09874340182034835\n",
      "0.651304332340991\n",
      "0.703516988152653\n",
      "0.6102408126466297\n",
      "0.799615261736028\n",
      "0.03457121987163425\n",
      "0.7702387345549799\n",
      "0.731728600739527\n",
      "0.2596983932983531\n",
      "0.2570692988221467\n",
      "0.6323033174301278\n",
      "0.34529746159108154\n",
      "0.7965886780072875\n",
      "0.44614623200345727\n",
      "0.7827494147841719\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(rdm.rand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1 0.1 0.1 0.1 0.1]\n"
     ]
    }
   ],
   "source": [
    "bias=tf.Variable(tf.constant(0.1,shape=[5]))#创造矩阵[0.1 0.1 0.1 0.1 0.1] 自动填补\n",
    "with tf.Session() as sess:\n",
    "    init_op=tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    print(sess.run(bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
