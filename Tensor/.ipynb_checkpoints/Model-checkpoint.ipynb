{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "INPUT_NODE=784\n",
    "OUTPUT_NODE=10\n",
    "\n",
    "IMAGE_SIZE=28\n",
    "NUM_CHANNELS=1\n",
    "NUM_LABELS=10\n",
    "\n",
    "CONV1_DEEP=32\n",
    "CONV1_SIZE=5\n",
    "\n",
    "CONV2_DEEP=64\n",
    "CONV2_SIZE=5\n",
    "\n",
    "FC_SIZE=512\n",
    "\n",
    "#参数train用于区分训练过程还是测试过程\n",
    "def inference(input_tensor,train,regularizer):\n",
    "    # 声明第一层卷积层，输入28*28*1，输出28*28*32\n",
    "    with tf.variable_scope('layer1_conv1'):\n",
    "        conv1_Weight=tf.get_variable('Weight',[CONV1_SIZE,CONV1_SIZE,NUM_CHANNELS,CONV1_DEEP],\n",
    "                                     initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv1_biase=tf.get_variable('biase',[CONV1_DEEP],initializer=tf.constant_initializer(0.0))\n",
    "        conv1=tf.nn.conv2d(input_tensor,conv1_Weight,strides=[1,1,1,1],padding='SAME')\n",
    "        relu1=tf.nn.relu(tf.nn.bias_add(conv1,conv1_biase))\n",
    "\n",
    "    # 声明第一层池化层，输入28*28*32，输出14*14*32\n",
    "    with tf.variable_scope('layer2_pool1'):\n",
    "        pool1=tf.nn.max_pool(relu1,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "    # 声明第三层卷积层，输入14*14*32，输出14*14*64\n",
    "    with tf.variable_scope('layer3_conv2'):\n",
    "        conv2_Weight=tf.get_variable('Weight',[CONV2_SIZE,CONV2_SIZE,CONV1_DEEP,CONV2_DEEP],\n",
    "                                     initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv2_biase=tf.get_variable('biase',[CONV2_DEEP],initializer=tf.constant_initializer(0.0))\n",
    "        conv2=tf.nn.conv2d(pool1,conv2_Weight,strides=[1,1,1,1],padding='SAME')\n",
    "        relu2=tf.nn.relu(tf.nn.bias_add(conv2,conv2_biase))\n",
    "\n",
    "    # 声明第四层池化层，输入14*14*64，输出7*7*64\n",
    "    with tf.variable_scope('layer4_pool2'):\n",
    "        pool2=tf.nn.max_pool(relu2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "    # 将第四层输出格式（7*7*64)转化为第五层的输入格式一个向\n",
    "    pool_shape=pool2.get_shape().as_list()\n",
    "    #pool_shape[0]存储的是一个batch中的数据个数\n",
    "    nodes=pool_shape[1]*pool_shape[2]*pool_shape[3]\n",
    "    reshaped=tf.reshape(pool2,[pool_shape[0],nodes])\n",
    "\n",
    "    # 声明第五层全连接层，输入7*7*64=3136长度的向量，输出512\n",
    "    with tf.variable_scope('layer5_fc1'):\n",
    "        fc1_Weight=tf.get_variable('Weight',[nodes,FC_SIZE],\n",
    "                                   initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        #只有全连接层的权重需要加入正则化\n",
    "        if regularizer!=None:\n",
    "            tf.add_to_collection('losses',regularizer(fc1_Weight))\n",
    "        fc1_biase=tf.get_variable('biase',[FC_SIZE],initializer=tf.constant_initializer(0.1))\n",
    "        fc1=tf.nn.relu(tf.matmul(reshaped,fc1_Weight)+fc1_biase)\n",
    "        if train:\n",
    "            #用于防止过拟合,dropout随机将部分节点输出改为0，一般只在全连接层使用\n",
    "            fc1=tf.nn.dropout(fc1,0.5)\n",
    "\n",
    "    # 声明第6层全连接层，输入512，输出10，通过softmax之后得到最后的分类结果\n",
    "    with tf.variable_scope('layer6_fc2'):\n",
    "        fc2_Weight=tf.get_variable('Weight',[FC_SIZE,NUM_LABELS],\n",
    "                                   initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        #只有全连接的权重需要加入正则化\n",
    "        if regularizer!=None:\n",
    "            tf.add_to_collection('losses',regularizer(fc2_Weight))\n",
    "        fd2_biase=tf.get_variable('biase',[NUM_LABELS],initializer=tf.constant_initializer(0.1))\n",
    "        prediction=tf.nn.relu(tf.matmul(fc1,fc2_Weight)+fd2_biase)\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting D:/PythonCode/Machine/jupyter1/Tensor/path/to/MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting D:/PythonCode/Machine/jupyter1/Tensor/path/to/MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting D:/PythonCode/Machine/jupyter1/Tensor/path/to/MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting D:/PythonCode/Machine/jupyter1/Tensor/path/to/MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "After 1 training steps, loss on training batch is 3.28299\n",
      "After 101 training steps, loss on training batch is 2.24925\n",
      "After 201 training steps, loss on training batch is 1.95208\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-12e50bad6d49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D:/PythonCode/Machine/jupyter1/Tensor/path/to/MNIST_data'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-12e50bad6d49>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(mnist)\u001b[0m\n\u001b[0;32m     57\u001b[0m                                    \u001b[0mIMAGE_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m                                    NUM_CHANNELS])\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mreshape_xs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mys\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'After %d training steps, loss on training batch is %g'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mH:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "BATCH_SIZE=100\n",
    "LAERNING_RATE_BASE=0.001\n",
    "LEARNING_RATE_DECAY=0.99\n",
    "TRAINING_STEPS=1000\n",
    "MOVING_AVERAGE_DECAY=0.99\n",
    "#描述模型复杂度的正则化项在损失函数中的系数\n",
    "REGULARIZATION_RATE=0.0001\n",
    "\n",
    "MODEL_SAVE_PATH='D:/PythonCode/Machine/jupyter1/Tensor/path/to/MNIST_data'\n",
    "#MODEL_NAME='model.ckpt'\n",
    "\n",
    "def train(mnist):\n",
    "    #卷积神经网络的出入层为三层\n",
    "    X=tf.placeholder(tf.float32,[\n",
    "        BATCH_SIZE,                     #第一维表示一个batch中样例个数\n",
    "        IMAGE_SIZE,    #第二维和第三维表示图像的尺寸\n",
    "        IMAGE_SIZE,\n",
    "        NUM_CHANNELS   #图像的深度\n",
    "    ],name='x_input')\n",
    "    y=tf.placeholder(tf.float32,[None,OUTPUT_NODE],name='y_input')\n",
    "    regularizer=tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "\n",
    "    pre_y=inference(X,False,regularizer)\n",
    "\n",
    "    global_step=tf.Variable(0,trainable=False)\n",
    "    variable_average=tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY,global_step)\n",
    "    variable_average_op=variable_average.apply(tf.trainable_variables())\n",
    "\n",
    "    cross_entrogy=tf.nn.sparse_softmax_cross_entropy_with_logits(logits=pre_y,labels=tf.argmax(y,1))\n",
    "    cross_entrogy_mean=tf.reduce_mean(cross_entrogy)\n",
    "    loss=cross_entrogy_mean+tf.add_n(tf.get_collection('losses'))\n",
    "\n",
    "    learning_rate=tf.train.exponential_decay(LAERNING_RATE_BASE,global_step,\n",
    "                                             mnist.train.num_examples/BATCH_SIZE,LEARNING_RATE_DECAY)\n",
    "    train_step=tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "\n",
    "    with tf.control_dependencies([train_step,variable_average_op]):\n",
    "        train_op=tf.no_op(name='train')\n",
    "\n",
    "    #saver=tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        init=tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            Xs,ys=mnist.train.next_batch(BATCH_SIZE)\n",
    "            reshape_xs=np.reshape(Xs,\n",
    "                                  [BATCH_SIZE,\n",
    "                                   IMAGE_SIZE,\n",
    "                                   IMAGE_SIZE,\n",
    "                                   NUM_CHANNELS])\n",
    "            _,loss_value,step=sess.run([train_op,loss,global_step],feed_dict={X:reshape_xs,y:ys})\n",
    "            if i%100==0:\n",
    "                print('After %d training steps, loss on training batch is %g' % (step, loss_value))\n",
    "\n",
    "                #saver.save(sess,os.path.join(MODEL_SAVE_PATH,MODEL_NAME),global_step=global_step)\n",
    "\n",
    "                \n",
    "mnist=input_data.read_data_sets('D:/PythonCode/Machine/jupyter1/Tensor/path/to/MNIST_data',one_hot=True)\n",
    "train(mnist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
